Data oblivious programs the problem with data-oblivious programs is they incur a lot of performance overhead there are all previous works that showed this with a lot of workloads so this performance over hits made them impractical to use and one of the things one of the reasons for this performance overheads is overheads that are introduced because of loops So on with loops are optimized when loops are optimized that approach is if loops have a heuristic that lets them terminate the loop early on then that heuristic is no longer used in the data oblivious version because heuristic are mostly input dependent and input dependency is not allowed in data-oblivious programming so when loops are converted to data oblivious programs what we do is we turn We loop that previously terminated on some condition some criteria that's dependent on input will now run a fixed amount of iterations for every input so if you have for example a sorting algorithm that tries to sort an array but after every step it checks if the array is already sorted or not if it's sorted then the loop will terminate in the program will return the sorted array so something like both story but now converting to data oblivious our programs that's not allowed because you cannot read the input data that you're working on so that here's the will not be allowed so the loop will run the maximum iteration or the worst case iteration which for bubble sort for example might be the size of the array that's to be sorted so we will replace any while loop with a four loop that runs in number of times so that's how we convert loops but this means if loops are running and the number of times but that input data Causes the loop to terminate let's say half of N number of times that means the next half of and iterations are useless basically but we cannot let that information leak because it will give out information about on the input so we just iterate up to N iterations even though the second-half does not really do any So this is where the overhead is coming from in the case of data-oblivious programs that contain loops yeah so you can imagine if N is a very big number then you'll have a worst case performance for the loop every time you execute that program with regardless of the input right so our approach is these algorithms that are to be turned into or converted into data oblivious algorithms we're arguing that they don't need to run the maximum number of hydration or the worst case iteration every time and we came up with an approach that first profiles and algorithm with representative data sets for example if it's like a sorting algorithm numbers that are representative of like a real world use or if it's a searching algorithm data set that are representative of free word use like DNA data sets or dictionary data sets and so on and also randomly generated data set so we come up with this representative datasets and we profile it and see for each execution how many times is wild exciting so we come up with this distribution of loop iterations and let's say most inputs cause that loop to execute 90% of the maximum iteration then instead of running it at a Max iteration label why not run it at 90% iteration and then the remaining 10% of inputs like if we're really unlucky and we if we're lucky then there's like a 90% chance that all your executions will go through even though the upper bound is now lowered to 90% of the maximum iteration but if you're unlucky and if you if you encounter a more loops that wants to go beyond 90% of iteration then you can rerun it as a time but still the cumulative performance would be better than just running everything at 100% coverage so we call that coverage if your coverage is 100% then that means you're running the loop until the end until the worst case maximum iteration if it's fifty then you're running half towards the case iteration right so that's the insights or the approach we took in this in this problem.

So what we did was we identified different algorithms that have a while loop on. And the wide loop is dependent on the. The while loop condition is somehow dependent on the input data. And we identified representative data for each algorithm, and we profiled it. We run it. Uh, many, many times. And we all try to get an understanding of. If we run for example 10,000 executions with 10,000 different data, then how many are of the 1000 execution in how many of them the wide loop executed? Like full execution like 100% of the worst case or like we just we were just we just recorded the number of iterations for that for that loop so. So that's our workflow. Then after we recorded this,. We call it frequency or trip frequency. We call it uh, trip frequency. We try to plot this frequency as a histogram and see how many like. For example if the loop is supposed to run 3232 iterations, then out of the 10,000 executions.. How many of them run for example 16 iterations? How many of them run? 20 iterations. So we plotted and. Depending on the curve that we get, we can tell. If the program has a tendency of not fully utilizing that while loop or not needing to execute all the way to 100% of worst case trip counts. And then? That's, that's our that's our workflow. Once we get. A good number that we can use to as a cut off or as a new worst case or a new maximum iteration. Then we can generate fresh data. And then we run that algorithm with the freshly generated data and see how many of. This executions were able to. Go through. Like we're able to complete within the new upper bound or the new maximum iteration and how many resulted in an incorrect result because the program needed the while loop needed to execute more iterations but. We did not let it. Then. We. Cranked up the marks on my traction all the way to 100 and rerun these inputs again. So, what's convenient here is In data oblivious programming. We have the capacity to capture those cases that did not go through. So, if a specific set of inputs a specific input. caused the algorithm to fail. In our approach we can capture it, we can tell which data set wanted to execute more than the set maximum iteration and we can accumulate all those inputs and send that again to be executed at 100%. Of. The maximum iteration. And even in doing that, we still get. Performance boosts.
 so we did this experiment on a lot of different algorithms including Boyer Moore string surge Dijkstra shortest distance algorithm GCD list and so on and we were able to notice significant optimization opportunities and each of the mentioned algorithms and the next question we had to answer was why are these programs exhibiting this property or what is that's what is the core property that lets this algorithms be optimizable or that allows this approach of ours to work for this algorithms because not every algorithm can benefit from this for like for example if the loop in your program is always running a specific number of times or a fixed number of times then this approach cannot do anything for that loop because when you convert it just stays fixed it's already data oblivious if it's a fixed number of iterations but if you have a while loop that terminates based on a condition that's dependent on the input or if you have a loop any kind with an embedded break statement based on a condition or like any loop with an early exit based on the secrets or input value then there might be an opportunity to optimize there but not all algorithms even those which meet this criteria are not guaranteed to benefit from this optimization yeah so we looked in we looked into our algorithms different algorithms and we found out that it all comes down to heuristics so if the heuristic is a strong enough in the native version if the heuristic is a strong enough so that most inputs don't really need to go through the entire trip count then there's an opportunity to optimize there for example think of a sorting algorithm that on every step checks if the array is sorted or not if the algorithm is written in a way that most inputs are sorted within 70% of this steps then there's an opportunity to optimize there so this properties vary based on different algorithms but this is the common denominator this is the main idea if there's a heuristic that is very strong and lets most cases of execution to not need to iterate the maximum iteration or like the iterate the whole iteration of the loop then there's an opportunity to optimize there. so insight here would be data oblivious programming is so opposed to heuristics it's so opposed to heuristics if you have a heuristic that means like you're trying to personalized that execution based on the input data right so like if you don't need some iterations you cut some iterations if you don't need some branches you cut those branches so here's things do that but for data oblivious programming any decision made based on input data is prohibited so we don't like heuristics and data oblivious programming but this work actually uses heuristics in order to lower the performance overhead of data-oblivious programs so actually if you have a stronger or more intimate heuristic there's more opportunity for you to apply this optimization and benefit from the performance improvements uh so generally when algorithms of a certain class for example sorting algorithms when we compared the performance in native versions algorithms that have a lot of heuristic arts strong heuristic tend to be very fast and algorithms that just do not care about the input data are among the slowest but when it comes to data oblivious programming algorithms that have least amount of heretics are the ones that are the fastest for data oblivious programming because data oblivious transformations will strip away any heuristic that is used in the native version and that will introduce a lot of performance overhead while we've done here is we've given some advantage back to those algorithms with strong heuristics in order for them to be able to compete in data oblivious design space in some cases with our optimization they might be faster than implementation with no heuristic in their algorithm in their native algorithm.

So there are a lot of different algorithms that are being used and that would benefit from security and privacy guarantees that are offered by data-oblivious programs for example we can think of DNA genomic sequencing as a search algorithm since our health data is usually very sensitive genomic sequencing would benefit from data oblivious search algorithms and if you have an algorithm that you want to convert into data oblivious program we offer an opportunity to apply this approach and benefit from it so all you need to ask is your algorithm incorporating a heuristic that lets loops in your program to exit early based on the input data and does this here is stick work for the majority of your inputs data if the answer is yes then then you'll be able to benefit from this optimization but if the answer is no then if you don't have a heuristic lots of heuristics in your problem in your from or in your loop then probably you won't incur a lot of performance overheads from data previous transformations because the last heuristic the less overheads but you won't be able to use this optimization.




Boyer Moore string search

So the first algorithm we focused on is string search or string search algorithm space completely we looked at Boyer Moore string search algorithm now this is a strict search algorithm relies a lot on heuristics and it's actually called Boyer-moore with some kind of heuristics so we have a version that's called Boyer-moore with bad character heuristics, Boyer Moore with good suffix and there's turbo Boyer-moore which tries to combine both bad character and good suffix heuristics and as you can imagine the turbo Boyer-moore was the one that performs the best but this strong heuristics became the reason for the very high overhead of the data previous version of Boyer more algorithm so in a previous work we were able to demonstrate that this Boyer-moore algorithm the overhead is so high a simple iterative search algorithm outperforms the data-oblivious version of the heuristic based variant. so in order to profile this algorithm we consider the different classes of datasets the first case is genomic sequencing where string search algorithms are used for pattern matching so we'll have a text that's usually a complete reference on the DNA of an Organism and then different chunks of DNA or just a bunch of uh nucleotides that are supposed to be matched in different positions of the reference DNA so search algorithm is used to search that position and match that pattern into the text so in order to see on the frequency distribution of the iteration or the trip count we took DNA from different organisms including COVID-19 virus chimp DNA a dog DNA a human DNA and we run this search algorithm using millions of different patterns that are extracted from the same DNA so we did this with fixed size patterns for example if our DNA is 3,000,000 nucleotides long then we would take DNA chunks of size 7 nucleotide for example and try to match or try to find a position in that text which is the reference DNA and what we found is most of them do not need to go all the way to the maximum iteration count of the while loop in Boyer-moore algorithm implementation so 60% is actually the sweet spot for this implementation So what we did is we lowered this maximum trip count to 60% and we run it with fresh data and we got a significant performance advantage over the data-oblivious version with 100% trip count.

The second data set we used with this algorithm as text search in a simple dictionary so we took a Dictionary of all known English words and we tried to search every word in that dictionary against the dictionary so we took all the words in the dictionary they we pushed them together into one big text and then we took each word from the dictionary and try to search them like we randomize it the order and then we try to search them this also indicated a really nice frequency distribution for the trip count showing that there is an opportunity to optimize so we run the algorithm with uh fresh new data and we observe a really good performance improvement.

The third data set we tried was we took random characters that includes all English letters numbers and punctuations and we constructed a random text and a bunch of random characters and we did the search and this does not show a lot of opportunity for optimization and the insights we get from this is uh maybe real world data has some sort of structure in it like the DNA data for example which is used by a lot of institutes it has some sort of order or structure that leads to this optimization opportunities.

Dijkstra shortest distance 


